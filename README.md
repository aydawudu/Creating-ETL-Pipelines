
This project seeks to establish a guideline for creating data pipelines. Research shows that scientist/analyst spend more than 80% cleaning and prepping data. I have found that having a framework can make the process a bit smoother. The outline below showcase some of the steps to use in cleaning and transforming data. 

# Creating-ETL-Pipelines
# Outline
## 1.Extract data from different sources such as:
* csv files
* json files
* APIs

## 2.Transform data
* combining data from different sources
* data cleaning
* data types
* parsing dates
* file encodings
* missing data
* duplicate data
* dummy variables
* remove outliers
* scaling features
* engineering features
 
## 3.Load
* send the transformed data to a database

## 4. ETL Pipeline
* code an ETL pipeline

Tool: Jupyter Notebook

Adapted from Udacity Data Engineering Nanodegree
